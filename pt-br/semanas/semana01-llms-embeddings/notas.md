# Semana 1: Fundamentos PrÃ¡ticos de LLMs e Embeddings

## ğŸ¯ Objetivo Geral
Compreender na teoria e prÃ¡tica o que sÃ£o LLMs e embeddings, como gerÃ¡-los e usÃ¡-los para tarefas reais de similaridade semÃ¢ntica.

---

## ğŸ“… Cronograma DiÃ¡rio (7 dias)

### **Dia 1: Teoria dos LLMs**
**Tempo:** 30-45 min  
**Objetivo:** Entender o panorama atual dos LLMs

**Atividades:**
- [ ] ğŸ“– Ler: [What are Large Language Models (LLMs)?](https://aws.amazon.com/what-is/large-language-model/)
- [ ] ğŸ“– Ler: [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) (primeiros 50%)
- [ ] ğŸ“ Resumir em 3 parÃ¡grafos: O que sÃ£o LLMs e por que sÃ£o importantes
- [ ] ğŸ§  Listar 5 aplicaÃ§Ãµes prÃ¡ticas de LLMs que vocÃª conhece

**Resultado esperado:** VocabulÃ¡rio bÃ¡sico sobre LLMs e transformers

---

### **Dia 2: Teoria dos Embeddings**
**Tempo:** 30-45 min  
**Objetivo:** Entender o conceito de embeddings e suas aplicaÃ§Ãµes

**Atividades:**
- [ ] ğŸ“– Ler: [What are embeddings?](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)
- [ ] ğŸ“– Ler: [Word2Vec Explained](https://towardsdatascience.com/word2vec-explained-49c52b4ccb71)
- [ ] ğŸ“ Criar um mapa mental: tipos de embeddings (palavra, frase, documento)
- [ ] ğŸ§  Pensar em 3 casos de uso para embeddings no seu contexto profissional/pessoal

**Resultado esperado:** CompreensÃ£o clara de como embeddings capturam significado

---

### **Dia 3: Setup e Primeiro CÃ³digo**
**Tempo:** 45-60 min  
**Objetivo:** Configurar ambiente e gerar primeiros embeddings

**Atividades:**
- [ ] âš™ï¸ Setup do ambiente Python (requirements.txt)
- [ ] ğŸ‘¨â€ğŸ’» Criar `exemplo_01_basico.py`: Gerar embeddings com Sentence Transformers
- [ ] ğŸ§ª Testar com 3-5 frases simples
- [ ] ğŸ“Š Printar dimensÃµes e primeiros valores dos embeddings
- [ ] ğŸ“ Documentar observaÃ§Ãµes no arquivo `experimentos.md`

**Resultado esperado:** Ambiente funcionando + primeiros embeddings gerados

---

### **Dia 4: ComparaÃ§Ã£o de Similaridade**
**Tempo:** 45-60 min  
**Objetivo:** Calcular e interpretar similaridade semÃ¢ntica

**Atividades:**
- [ ] ğŸ‘¨â€ğŸ’» Criar `exemplo_02_similaridade.py`: calcular cosine similarity
- [ ] ğŸ§ª Testar com pares de frases: similares vs diferentes
- [ ] ğŸ“Š Criar uma tabela de resultados (frase1, frase2, similaridade)
- [ ] ğŸ” Investigar: Por que algumas frases tÃªm alta similaridade?
- [ ] ğŸ“ Documentar insights sobre limitaÃ§Ãµes e surpresas

**Resultado esperado:** CompreensÃ£o prÃ¡tica de como medir similaridade semÃ¢ntica

---

### **Dia 5: ComparaÃ§Ã£o de Modelos**
**Tempo:** 45-60 min  
**Objetivo:** Testar diferentes modelos de embeddings

**Atividades:**
- [ ] ğŸ‘¨â€ğŸ’» Criar `exemplo_03_comparacao_modelos.py`
- [ ] ğŸ§ª Testar pelo menos 3 modelos: 
  - `all-MiniLM-L6-v2` (rÃ¡pido)
  - `all-mpnet-base-v2` (melhor qualidade)
  - `distilbert-base-multilingual-cased` (multilingual)
- [ ] ğŸ“Š Comparar resultados de similaridade para o mesmo conjunto de frases
- [ ] â±ï¸ Medir tempo de processamento de cada modelo
- [ ] ğŸ“ AnÃ¡lise: trade-off entre velocidade e qualidade

**Resultado esperado:** Conhecimento prÃ¡tico das diferenÃ§as entre modelos

---

### **Dia 6: VisualizaÃ§Ã£o**
**Tempo:** 45-60 min  
**Objetivo:** Visualizar embeddings em 2D para intuiÃ§Ã£o geomÃ©trica

**Atividades:**
- [ ] ğŸ‘¨â€ğŸ’» Criar `exemplo_04_visualizacao.py`
- [ ] ğŸ“Š Usar t-SNE ou UMAP para reduzir dimensionalidade
- [ ] ğŸ¨ Plotar embeddings com matplotlib/plotly
- [ ] ğŸ·ï¸ Colorir pontos por categorias (se aplicÃ¡vel)
- [ ] ğŸ” Interpretar: clusters fazem sentido semÃ¢ntico?
- [ ] ğŸ“ Capturar screenshots dos plots

**Resultado esperado:** IntuiÃ§Ã£o visual de como embeddings organizam informaÃ§Ã£o semÃ¢ntica

---

### **Dia 7: Mini-Projeto Final**
**Tempo:** 60-90 min  
**Objetivo:** Consolidar aprendizado em projeto prÃ¡tico

**Atividades:**
- [ ] ğŸ‘¨â€ğŸ’» Criar `mini_projeto_busca_semantica.py`
- [ ] ğŸ“š Dataset: 20-30 frases sobre temas variados
- [ ] ğŸ” Implementar busca semÃ¢ntica: dado um query, retornar top-3 mais similares
- [ ] ğŸ“Š Interface simples: input do usuÃ¡rio â†’ resultados rankeados
- [ ] ğŸ§ª Testar com queries interessantes
- [ ] ğŸ“ README do mini-projeto com exemplos e resultados
- [ ] ğŸ‰ Celebrar! Documentar aprendizados gerais da semana

**Resultado esperado:** AplicaÃ§Ã£o funcional de busca semÃ¢ntica

---

## ğŸ“š Recursos de Estudo

### **Leituras Essenciais**
- [Embeddings â€” OpenAI](https://platform.openai.com/docs/guides/embeddings)
- [Sentence Transformers Documentation](https://www.sbert.net/)
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)

### **Leituras Complementares**
- [HuggingFace Transformers Course](https://huggingface.co/learn/nlp-course/chapter1/1)
- [OpenAI Cookbook: Embeddings](https://github.com/openai/openai-cookbook/blob/main/examples/Getting_embeddings.ipynb)
- [Awesome Embeddings Papers](https://github.com/Separius/awesome-sentence-embedding)

### **Comunidades**
- [HuggingFace Discord](https://discord.gg/JfAtkvEtRb)
- [r/MachineLearning](https://reddit.com/r/MachineLearning)
- [OpenAI Community](https://community.openai.com/)

---

## ğŸ› ï¸ Setup TÃ©cnico

### **DependÃªncias**
```bash
# Adicionar ao requirements.txt
sentence-transformers==2.2.2
numpy==1.24.3
matplotlib==3.7.1
scikit-learn==1.3.0
umap-learn==0.5.3
plotly==5.15.0
pandas==2.0.3
```

### **Estrutura de Arquivos**
```
semana01-llms-embeddings/
â”œâ”€â”€ notas.md                           # Este arquivo
â”œâ”€â”€ experimentos.md                    # Suas anotaÃ§Ãµes
â”œâ”€â”€ exemplos/
â”‚   â”œâ”€â”€ exemplo_01_basico.py
â”‚   â”œâ”€â”€ exemplo_02_similaridade.py
â”‚   â”œâ”€â”€ exemplo_03_comparacao_modelos.py
â”‚   â””â”€â”€ exemplo_04_visualizacao.py
â””â”€â”€ mini-projeto/
    â”œâ”€â”€ mini_projeto_busca_semantica.py
    â”œâ”€â”€ dataset_exemplo.txt
    â””â”€â”€ README.md
```

---

## âœ… Checklist de Aprendizado

### **Conceitos TeÃ³ricos**
- [ ] Consigo explicar o que sÃ£o LLMs em 2 minutos
- [ ] Entendo a diferenÃ§a entre embeddings de palavra vs frase vs documento
- [ ] Sei explicar cosine similarity e quando usar
- [ ] ConheÃ§o pelo menos 3 aplicaÃ§Ãµes prÃ¡ticas de embeddings

### **Habilidades PrÃ¡ticas**
- [ ] Consigo gerar embeddings com Sentence Transformers
- [ ] Sei calcular similaridade entre textos
- [ ] Testei pelo menos 3 modelos diferentes
- [ ] Criei visualizaÃ§Ã£o 2D de embeddings
- [ ] Implementei busca semÃ¢ntica bÃ¡sica

### **Projeto Final**
- [ ] Mini-projeto funciona corretamente
- [ ] Documentei cÃ³digo e resultados
- [ ] Identifiquei limitaÃ§Ãµes e prÃ³ximos passos

---

## ğŸ“ EspaÃ§o para AnotaÃ§Ãµes

### **DÃºvidas**
- (Anote aqui suas dÃºvidas para pesquisar depois)

### **Insights**
- (O que mais te surpreendeu? O que funcionou melhor/pior que esperado?)

### **Ideias de Projetos**
- (InspiraÃ§Ãµes para projetos futuros usando embeddings)

### **Links Extras**
- (Recursos Ãºteis que encontrou durante a semana)